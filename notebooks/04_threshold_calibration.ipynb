{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284eced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# Day 4 â€“ Threshold Tuning & Calibration\\n',\n",
       "    '\\n',\n",
       "    'In fraud detection, class imbalance makes **threshold = 0.5** unreliable.\\n',\n",
       "    '- We tune the threshold for **F1 (fraud class)**.\\n',\n",
       "    '- We add **Precision-Recall curves** and **calibration curves**.\\n',\n",
       "    '- This improves both **decision-making** (when to flag fraud) and **probability quality** (confidence in fraud risk).']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 1,\n",
       "   'outputs': [],\n",
       "   'source': ['import pandas as pd\\n',\n",
       "    'import matplotlib.pyplot as plt\\n',\n",
       "    'from pathlib import Path\\n',\n",
       "    'import joblib\\n',\n",
       "    'from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\\n',\n",
       "    'from sklearn.calibration import calibration_curve\\n',\n",
       "    'from src.data_prep import load_raw, get_feature_target, build_preprocessor\\n',\n",
       "    'from src.evaluate import evaluate_classification, find_best_threshold']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Load dataset and trained model']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 2,\n",
       "   'outputs': [],\n",
       "   'source': ['df = load_raw(\"../data/raw/creditcard.csv\")\\n',\n",
       "    'X, y = get_feature_target(df)\\n',\n",
       "    '\\n',\n",
       "    '# load trained Random Forest model for demo\\n',\n",
       "    'model_path = Path(\"../models/rf_pipeline.joblib\")\\n',\n",
       "    'model = joblib.load(model_path)\\n',\n",
       "    '\\n',\n",
       "    'y_proba = model.predict_proba(X)[:, 1]']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Baseline metrics (threshold = 0.5)']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 3,\n",
       "   'outputs': [],\n",
       "   'source': ['metrics_05 = evaluate_classification(y, y_proba, threshold=0.5)\\n',\n",
       "    'metrics_05']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Find best threshold (optimize fraud F1)']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 4,\n",
       "   'outputs': [],\n",
       "   'source': ['best_thr = find_best_threshold(y, y_proba, metric=\"f1\")\\n',\n",
       "    'best_thr']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Precision-Recall Curve']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 5,\n",
       "   'outputs': [],\n",
       "   'source': ['prec, rec, thr = precision_recall_curve(y, y_proba)\\n',\n",
       "    'ap = average_precision_score(y, y_proba)\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(6, 4))\\n',\n",
       "    'plt.plot(rec, prec, label=f\"AP = {ap:.4f}\")\\n',\n",
       "    'plt.xlabel(\"Recall\")\\n',\n",
       "    'plt.ylabel(\"Precision\")\\n',\n",
       "    'plt.title(\"Precision-Recall Curve\")\\n',\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown', 'metadata': {}, 'source': ['## ROC Curve']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 6,\n",
       "   'outputs': [],\n",
       "   'source': ['fpr, tpr, _ = roc_curve(y, y_proba)\\n',\n",
       "    'roc_auc = auc(fpr, tpr)\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(6, 4))\\n',\n",
       "    'plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.4f}\")\\n',\n",
       "    'plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\")\\n',\n",
       "    'plt.xlabel(\"False Positive Rate\")\\n',\n",
       "    'plt.ylabel(\"True Positive Rate\")\\n',\n",
       "    'plt.title(\"ROC Curve\")\\n',\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Calibration Curve (probability quality)']},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {},\n",
       "   'execution_count': 7,\n",
       "   'outputs': [],\n",
       "   'source': ['prob_true, prob_pred = calibration_curve(y, y_proba, n_bins=10, strategy=\"uniform\")\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(6, 4))\\n',\n",
       "    'plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration curve\")\\n',\n",
       "    'plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\", label=\"Perfect calibration\")\\n',\n",
       "    'plt.xlabel(\"Mean predicted probability\")\\n',\n",
       "    'plt.ylabel(\"Fraction of positives\")\\n',\n",
       "    'plt.title(\"Calibration Curve\")\\n',\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### ðŸ“Œ Insights\\n',\n",
       "    '- Default threshold (0.5) is not optimal.\\n',\n",
       "    '- Tuned threshold increases **fraud recall & F1**.\\n',\n",
       "    '- Precision-Recall curve is more informative than ROC in imbalanced datasets.\\n',\n",
       "    '- Calibration curves show whether fraud probabilities are **trustworthy**.']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'name': 'python', 'version': '3.11'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 5}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Day 4 â€“ Threshold Tuning & Calibration\\n\",\n",
    "    \"\\n\",\n",
    "    \"In fraud detection, class imbalance makes **threshold = 0.5** unreliable.\\n\",\n",
    "    \"- We tune the threshold for **F1 (fraud class)**.\\n\",\n",
    "    \"- We add **Precision-Recall curves** and **calibration curves**.\\n\",\n",
    "    \"- This improves both **decision-making** (when to flag fraud) and **probability quality** (confidence in fraud risk).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 1,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\\n\",\n",
    "    \"from sklearn.calibration import calibration_curve\\n\",\n",
    "    \"from src.data_prep import load_raw, get_feature_target, build_preprocessor\\n\",\n",
    "    \"from src.evaluate import evaluate_classification, find_best_threshold\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Load dataset and trained model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 2,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df = load_raw(\\\"../data/raw/creditcard.csv\\\")\\n\",\n",
    "    \"X, y = get_feature_target(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# load trained Random Forest model for demo\\n\",\n",
    "    \"model_path = Path(\\\"../models/rf_pipeline.joblib\\\")\\n\",\n",
    "    \"model = joblib.load(model_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"y_proba = model.predict_proba(X)[:, 1]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Baseline metrics (threshold = 0.5)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 3,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"metrics_05 = evaluate_classification(y, y_proba, threshold=0.5)\\n\",\n",
    "    \"metrics_05\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Find best threshold (optimize fraud F1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 4,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"best_thr = find_best_threshold(y, y_proba, metric=\\\"f1\\\")\\n\",\n",
    "    \"best_thr\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Precision-Recall Curve\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 5,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"prec, rec, thr = precision_recall_curve(y, y_proba)\\n\",\n",
    "    \"ap = average_precision_score(y, y_proba)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(6, 4))\\n\",\n",
    "    \"plt.plot(rec, prec, label=f\\\"AP = {ap:.4f}\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Recall\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Precision\\\")\\n\",\n",
    "    \"plt.title(\\\"Precision-Recall Curve\\\")\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ROC Curve\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 6,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"fpr, tpr, _ = roc_curve(y, y_proba)\\n\",\n",
    "    \"roc_auc = auc(fpr, tpr)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(6, 4))\\n\",\n",
    "    \"plt.plot(fpr, tpr, label=f\\\"ROC AUC = {roc_auc:.4f}\\\")\\n\",\n",
    "    \"plt.plot([0,1],[0,1], linestyle=\\\"--\\\", color=\\\"gray\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"False Positive Rate\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"True Positive Rate\\\")\\n\",\n",
    "    \"plt.title(\\\"ROC Curve\\\")\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Calibration Curve (probability quality)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"execution_count\": 7,\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"prob_true, prob_pred = calibration_curve(y, y_proba, n_bins=10, strategy=\\\"uniform\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(6, 4))\\n\",\n",
    "    \"plt.plot(prob_pred, prob_true, marker=\\\"o\\\", label=\\\"Calibration curve\\\")\\n\",\n",
    "    \"plt.plot([0,1],[0,1], linestyle=\\\"--\\\", color=\\\"gray\\\", label=\\\"Perfect calibration\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Mean predicted probability\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Fraction of positives\\\")\\n\",\n",
    "    \"plt.title(\\\"Calibration Curve\\\")\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ“Œ Insights\\n\",\n",
    "    \"- Default threshold (0.5) is not optimal.\\n\",\n",
    "    \"- Tuned threshold increases **fraud recall & F1**.\\n\",\n",
    "    \"- Precision-Recall curve is more informative than ROC in imbalanced datasets.\\n\",\n",
    "    \"- Calibration curves show whether fraud probabilities are **trustworthy**.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.11\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
